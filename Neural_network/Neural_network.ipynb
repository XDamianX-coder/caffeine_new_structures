{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae12710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "My current directory is : C:\\Users\\aleks\\Documents\\GitHub\\caffeine_new_structures\\Neural_network\n",
      "My directory name is : Neural_network\n",
      "(118386,)\n",
      "(13154,)\n",
      "                                                   SMILES\n",
      "99343   N(CCN1C(CCC1=O)=O)C2=NC=3N(C)C(=O)N(C(=O)C=3N2C)C\n",
      "136644  S/C(C(OC)=O)=C\\C(=CN(C)CCCCCCN(C)C1=NC2=C(N1C)...\n",
      "236654  N1=C(N(C=2C(N(C(N(C)C1=2)=O)C)=O)C)NCC/N=C3/SC...\n",
      "131075  C(CCCNC=1N(C)C=2C(N(C(=O)N(C)C=2N=1)C)=O)NC(CS...\n",
      "250375        C(NC1=NC=2N(C)C(N(C(C=2N1C)=O)C)=O)CNC(=O)C\n",
      "163202  CN1C(N(C(=O)C=2N(C(NCCN3C(=O)C=CC3=O)=NC1=2)C)...\n",
      "307481  C(SCC(=O)NCCCNC=1N(C)C=2C(N(C)C(=O)N(C)C=2N=1)...\n",
      "125684  N(C=1N(C2=C(N(C)C(=O)N(C2=O)C)N=1)C)CC/N=C3\\NC...\n",
      "138992  C(N(C)C1=NC=2N(C(N(C(=O)C=2N1C)C)=O)C)CCCCCN(C...\n",
      "39173   C(CN1C(=O)C2=CC=CC=C2C1=O)NC=3N(C)C4=C(N=3)N(C...\n",
      "All works correct, encoding leads to the same data during decoding...\n",
      "{'[=N+1]': '#', '[C]': '$', '[N]': '%', '[N-1]': '&', '[=Ring1]': \"'\", '[\\\\S]': '(', '[\\\\N]': ')', '[O-1]': '*', '[Ring2]': '+', '[#C]': ',', '[/S]': '-', '[#Branch2]': '.', '[=C]': '/', '[=Ring2]': '0', '[=S]': '1', '[=Branch2]': '2', '[#Branch1]': '3', '[Branch1]': '4', '[=O]': '5', '[S]': '6', '[=Branch1]': '7', '[P]': '8', '[=N-1]': '9', '[O]': ':', '[/N]': ';', '[Ring1]': '<', '[/C]': '=', '[\\\\C]': '>', '[N+1]': '@', '[=N]': 'A', '[NH1]': 'B', '[Branch2]': 'C'}\n",
      "{'3', '2', ':', ')', '#', 'B', 'A', '+', '@', '(', '.', ';', '*', 'C', 'E', '/', '5', '!', '7', '0', '$', '-', '1', '8', '>', '9', '%', '=', \"'\", '&', ',', '6', '<', '4'}\n",
      "34 88\n",
      "C12=C(N(C(=O)N(C)C1=O)C)N=C(N2C)NCCCC/N=C3/SCC(N3)=O\n",
      "!%4/$$%$43$$$<455$A$%4$$$7$5%4%$7$5$'2%<%$$EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "[N][Branch1][=C][C][C][N][C][Branch1][#Branch1][C][C][C][Ring1][Branch1][=O][=O][C][=N][C][N][Branch1][C][C][C][=Branch1][C][=O][N][Branch1][N][C][=Branch1][C][=O][C][=Ring1][=Branch2][N][Ring1][N][C][C]\n",
      "[N][Branch1][=C][C][C][N][C][Branch1][#Branch1][C][C][C][Ring1][Branch1][=O][=O][C][=N][C][N][Branch1][C][C][C][=Branch1][C][=O][N][Branch1][N][C][=Branch1][C][=O][C][=Ring1][=Branch2][N][Ring1][N][C][C]\n",
      "Correct encoding-decoding: True\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 87, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 83456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 87, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 87, 128)      83456       input_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 87, 34)       4386        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 237,218\n",
      "Trainable params: 237,218\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "463/463 [==============================] - 17s 25ms/step - loss: 0.9273 - val_loss: 0.5489\n",
      "Epoch 2/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.4157 - val_loss: 0.3413\n",
      "Epoch 3/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.3177 - val_loss: 0.2865\n",
      "Epoch 4/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2673 - val_loss: 0.2533\n",
      "Epoch 5/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2425 - val_loss: 0.2358\n",
      "Epoch 6/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2272 - val_loss: 0.2212\n",
      "Epoch 7/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2177 - val_loss: 0.2142\n",
      "Epoch 8/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2097 - val_loss: 0.2105\n",
      "Epoch 9/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2028 - val_loss: 0.2008\n",
      "Epoch 10/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.2009 - val_loss: 0.1966\n",
      "Epoch 11/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1932 - val_loss: 0.1955\n",
      "Epoch 12/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1910 - val_loss: 0.1913\n",
      "Epoch 13/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1874 - val_loss: 0.1865\n",
      "Epoch 14/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1835 - val_loss: 0.1847\n",
      "Epoch 15/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1819 - val_loss: 0.1834\n",
      "Epoch 16/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1800 - val_loss: 0.1802\n",
      "Epoch 17/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1769 - val_loss: 0.1800\n",
      "Epoch 18/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1872 - val_loss: 0.2563\n",
      "Epoch 19/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1880 - val_loss: 0.1781\n",
      "Epoch 20/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1726 - val_loss: 0.1703\n",
      "Epoch 21/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1661 - val_loss: 0.1920\n",
      "Epoch 22/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1629 - val_loss: 0.1617\n",
      "Epoch 23/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1579 - val_loss: 0.1576\n",
      "Epoch 24/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1532 - val_loss: 0.1534\n",
      "Epoch 25/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1497 - val_loss: 0.1477\n",
      "Epoch 26/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.1432 - val_loss: 0.1406\n",
      "Epoch 27/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.1356 - val_loss: 0.1364\n",
      "Epoch 28/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.1306 - val_loss: 0.1327\n",
      "Epoch 29/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1263 - val_loss: 0.1283\n",
      "Epoch 30/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1223 - val_loss: 0.1234\n",
      "Epoch 31/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1192 - val_loss: 0.1233\n",
      "Epoch 32/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1174 - val_loss: 0.1147\n",
      "Epoch 33/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1123 - val_loss: 0.1181\n",
      "Epoch 34/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1110 - val_loss: 0.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 36/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1061 - val_loss: 0.1223\n",
      "Epoch 37/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1065 - val_loss: 0.1077\n",
      "Epoch 38/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1015 - val_loss: 0.1037\n",
      "Epoch 39/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1017 - val_loss: 0.1006\n",
      "Epoch 40/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0987 - val_loss: 0.1033\n",
      "Epoch 41/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.1027 - val_loss: 0.0984\n",
      "Epoch 42/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0968 - val_loss: 0.1050\n",
      "Epoch 43/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0944 - val_loss: 0.1167\n",
      "Epoch 44/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0949 - val_loss: 0.0950\n",
      "Epoch 45/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0906 - val_loss: 0.0960\n",
      "Epoch 46/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0911 - val_loss: 0.0930\n",
      "Epoch 47/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0891 - val_loss: 0.0898\n",
      "Epoch 48/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0886 - val_loss: 0.0931\n",
      "Epoch 49/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0884 - val_loss: 0.0924\n",
      "Epoch 50/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0848 - val_loss: 0.0889\n",
      "Epoch 51/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0853 - val_loss: 0.0876\n",
      "Epoch 52/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0839 - val_loss: 0.0906\n",
      "Epoch 53/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0835 - val_loss: 0.0893\n",
      "Epoch 54/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0878 - val_loss: 0.0844\n",
      "Epoch 55/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0826 - val_loss: 0.0851\n",
      "Epoch 56/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0806 - val_loss: 0.0877\n",
      "Epoch 57/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0799 - val_loss: 0.0828\n",
      "Epoch 58/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.0792 - val_loss: 0.0824\n",
      "Epoch 59/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0776 - val_loss: 0.0811\n",
      "Epoch 60/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0807 - val_loss: 0.0801\n",
      "Epoch 61/200\n",
      "463/463 [==============================] - 10s 23ms/step - loss: 0.0779 - val_loss: 0.0849\n",
      "Epoch 62/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0764 - val_loss: 0.0778\n",
      "Epoch 63/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0753 - val_loss: 0.0784\n",
      "Epoch 64/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0750 - val_loss: 0.0783\n",
      "Epoch 65/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0742 - val_loss: 0.0793\n",
      "Epoch 66/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0733 - val_loss: 0.0860\n",
      "Epoch 67/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0724 - val_loss: 0.0767\n",
      "Epoch 68/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0731 - val_loss: 0.0745\n",
      "Epoch 69/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0732 - val_loss: 0.0771\n",
      "Epoch 70/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0697 - val_loss: 0.0735\n",
      "Epoch 71/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0706 - val_loss: 0.0714\n",
      "Epoch 72/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0693 - val_loss: 0.0806\n",
      "Epoch 73/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0703 - val_loss: 0.0814\n",
      "Epoch 74/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0737 - val_loss: 0.0727\n",
      "Epoch 75/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0681 - val_loss: 0.0757\n",
      "Epoch 76/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0690 - val_loss: 0.0706\n",
      "Epoch 77/200\n",
      "463/463 [==============================] - 10s 23ms/step - loss: 0.0660 - val_loss: 0.0712\n",
      "Epoch 78/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0670 - val_loss: 0.0753\n",
      "Epoch 79/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0664 - val_loss: 0.0734\n",
      "Epoch 80/200\n",
      "463/463 [==============================] - 10s 23ms/step - loss: 0.0663 - val_loss: 0.0783\n",
      "Epoch 81/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0633 - val_loss: 0.0678\n",
      "Epoch 82/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0645 - val_loss: 0.0677\n",
      "Epoch 83/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0666 - val_loss: 0.0646\n",
      "Epoch 84/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0622 - val_loss: 0.0706\n",
      "Epoch 85/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0648 - val_loss: 0.0684\n",
      "Epoch 86/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0618 - val_loss: 0.0661\n",
      "Epoch 87/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0633 - val_loss: 0.0640\n",
      "Epoch 88/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0605 - val_loss: 0.0706\n",
      "Epoch 89/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0612 - val_loss: 0.0699\n",
      "Epoch 90/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0602 - val_loss: 0.0669\n",
      "Epoch 91/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0606 - val_loss: 0.0712\n",
      "Epoch 92/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0606 - val_loss: 0.0657\n",
      "Epoch 93/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0586 - val_loss: 0.0627\n",
      "Epoch 94/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0594 - val_loss: 0.0603\n",
      "Epoch 95/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0572 - val_loss: 0.0653\n",
      "Epoch 96/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0578 - val_loss: 0.0665\n",
      "Epoch 97/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0575 - val_loss: 0.0582\n",
      "Epoch 98/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0534 - val_loss: 0.0554\n",
      "Epoch 99/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0544 - val_loss: 0.0679\n",
      "Epoch 100/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0523 - val_loss: 0.0614\n",
      "Epoch 101/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0528 - val_loss: 0.0568\n",
      "Epoch 102/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0525 - val_loss: 0.0563\n",
      "Epoch 103/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0527 - val_loss: 0.0591\n",
      "Epoch 104/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0521 - val_loss: 0.0553\n",
      "Epoch 105/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0512 - val_loss: 0.0544\n",
      "Epoch 106/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0515 - val_loss: 0.0565\n",
      "Epoch 107/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0544 - val_loss: 0.0505\n",
      "Epoch 108/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0485 - val_loss: 0.0541\n",
      "Epoch 109/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0499 - val_loss: 0.0535\n",
      "Epoch 110/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0501 - val_loss: 0.0546\n",
      "Epoch 111/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0498 - val_loss: 0.0578\n",
      "Epoch 112/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0485 - val_loss: 0.0538\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0497 - val_loss: 0.0570\n",
      "Epoch 114/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0474 - val_loss: 0.0537\n",
      "Epoch 115/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0477 - val_loss: 0.0560\n",
      "Epoch 116/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0473 - val_loss: 0.0497\n",
      "Epoch 117/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0504 - val_loss: 0.0531\n",
      "Epoch 118/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0474 - val_loss: 0.0506\n",
      "Epoch 119/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0461 - val_loss: 0.0524\n",
      "Epoch 120/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0462 - val_loss: 0.0478\n",
      "Epoch 121/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0456 - val_loss: 0.0476\n",
      "Epoch 122/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0449 - val_loss: 0.0573\n",
      "Epoch 123/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0457 - val_loss: 0.0541\n",
      "Epoch 124/200\n",
      "463/463 [==============================] - 10s 23ms/step - loss: 0.0449 - val_loss: 0.0541\n",
      "Epoch 125/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0451 - val_loss: 0.0465\n",
      "Epoch 126/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0456 - val_loss: 0.0515\n",
      "Epoch 127/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0427 - val_loss: 0.0493\n",
      "Epoch 128/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0437 - val_loss: 0.0453\n",
      "Epoch 129/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0421 - val_loss: 0.0473\n",
      "Epoch 130/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0436 - val_loss: 0.0437\n",
      "Epoch 131/200\n",
      "463/463 [==============================] - 10s 23ms/step - loss: 0.0415 - val_loss: 0.0468\n",
      "Epoch 132/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0431 - val_loss: 0.0510\n",
      "Epoch 133/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0413 - val_loss: 0.0472\n",
      "Epoch 134/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0414 - val_loss: 0.0445\n",
      "Epoch 135/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0409 - val_loss: 0.0465\n",
      "Epoch 136/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0428 - val_loss: 0.0454\n",
      "Epoch 137/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0395 - val_loss: 0.0453\n",
      "Epoch 138/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0403 - val_loss: 0.0498\n",
      "Epoch 139/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0400 - val_loss: 0.0449\n",
      "Epoch 140/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0395 - val_loss: 0.0452\n",
      "Epoch 141/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0420 - val_loss: 0.0442\n",
      "Epoch 142/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0423 - val_loss: 0.0409\n",
      "Epoch 143/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0379 - val_loss: 0.0503\n",
      "Epoch 144/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0389 - val_loss: 0.0416\n",
      "Epoch 145/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0385 - val_loss: 0.0426\n",
      "Epoch 146/200\n",
      "463/463 [==============================] - 10s 22ms/step - loss: 0.0389 - val_loss: 0.0448\n",
      "Epoch 147/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0387 - val_loss: 0.0432\n",
      "Epoch 148/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.0390 - val_loss: 0.0518\n",
      "Epoch 149/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 150/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0408 - val_loss: 0.0483\n",
      "Epoch 151/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0372 - val_loss: 0.0405\n",
      "Epoch 152/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0383 - val_loss: 0.0610\n",
      "Epoch 153/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0381 - val_loss: 0.0418\n",
      "Epoch 154/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0375 - val_loss: 0.0409\n",
      "Epoch 155/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0385 - val_loss: 0.0442\n",
      "Epoch 156/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0353 - val_loss: 0.0377\n",
      "Epoch 157/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0389 - val_loss: 0.0373\n",
      "Epoch 158/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0346 - val_loss: 0.0402\n",
      "Epoch 159/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0362 - val_loss: 0.0414\n",
      "Epoch 160/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0354 - val_loss: 0.0396\n",
      "Epoch 161/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0364 - val_loss: 0.0456\n",
      "Epoch 162/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0362 - val_loss: 0.0531\n",
      "Epoch 163/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0370 - val_loss: 0.0392\n",
      "Epoch 164/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0355 - val_loss: 0.0402\n",
      "Epoch 165/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0360 - val_loss: 0.0391\n",
      "Epoch 166/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0355 - val_loss: 0.0538\n",
      "Epoch 167/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0347 - val_loss: 0.0463\n",
      "Epoch 168/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0347 - val_loss: 0.0472\n",
      "Epoch 169/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0364 - val_loss: 0.0393\n",
      "Epoch 170/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0337 - val_loss: 0.0414\n",
      "Epoch 171/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0350 - val_loss: 0.0393\n",
      "Epoch 172/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0349 - val_loss: 0.0366\n",
      "Epoch 173/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0353 - val_loss: 0.0395\n",
      "Epoch 174/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0337 - val_loss: 0.0394\n",
      "Epoch 175/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0340 - val_loss: 0.0374\n",
      "Epoch 176/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0331 - val_loss: 0.0362\n",
      "Epoch 177/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0370 - val_loss: 0.0434\n",
      "Epoch 178/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0382 - val_loss: 0.0344\n",
      "Epoch 179/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0306 - val_loss: 0.0411\n",
      "Epoch 180/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0330 - val_loss: 0.0372\n",
      "Epoch 181/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0321 - val_loss: 0.0375\n",
      "Epoch 182/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0346 - val_loss: 0.0406\n",
      "Epoch 183/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0350 - val_loss: 0.0369\n",
      "Epoch 184/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0322 - val_loss: 0.0445\n",
      "Epoch 185/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0317 - val_loss: 0.0360\n",
      "Epoch 186/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0333 - val_loss: 0.0536\n",
      "Epoch 187/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0323 - val_loss: 0.0370\n",
      "Epoch 188/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0333 - val_loss: 0.0358\n",
      "Epoch 189/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0305 - val_loss: 0.0378\n",
      "Epoch 190/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0314 - val_loss: 0.0368\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0331 - val_loss: 0.0354\n",
      "Epoch 192/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0310 - val_loss: 0.0364\n",
      "Epoch 193/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0325 - val_loss: 0.0340\n",
      "Epoch 194/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0294 - val_loss: 0.0360\n",
      "Epoch 195/200\n",
      "463/463 [==============================] - 11s 24ms/step - loss: 0.0315 - val_loss: 0.0367\n",
      "Epoch 196/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0319 - val_loss: 0.0361\n",
      "Epoch 197/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0310 - val_loss: 0.0372\n",
      "Epoch 198/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0305 - val_loss: 0.0325\n",
      "Epoch 199/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0304 - val_loss: 0.0427\n",
      "Epoch 200/200\n",
      "463/463 [==============================] - 11s 23ms/step - loss: 0.0302 - val_loss: 0.0331\n",
      "The time of execution of above program is : 2118.714285850525\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(1, 1, 34)]              0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, 1, 128)               83456     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 1, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 87,842\n",
      "Trainable params: 87,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "The time of execution of above program is : 2196.9818687438965\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "SELFIES_coder_path = Path(\"../SELFIES_coder\")\n",
    "sys.path.append(SELFIES_coder_path.as_posix())\n",
    "import SELFIES_coder as SELFIES_CODER\n",
    "import selfies as sf\n",
    "\n",
    "SELFIES_CODER.TEST()\n",
    "\n",
    "# Inital time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Know where user actually is\n",
    "directory_path = os.getcwd()\n",
    "print(\"My current directory is : \" + directory_path)\n",
    "folder_name = os.path.basename(directory_path)\n",
    "print(\"My directory name is : \" + folder_name)\n",
    "\n",
    "#load file with smiles codes to train and test (in this case one column is enough)\n",
    "smifile = ('../Data/training_data_smiles_caffeine_reduced.parquet')\n",
    "\n",
    "data = pd.read_parquet(smifile)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "smiles_train, smiles_test = train_test_split(data[\"SMILES\"], test_size=0.1, train_size=0.9, random_state=42)\n",
    "print(smiles_train.shape)\n",
    "print(smiles_test.shape)\n",
    "\n",
    "\n",
    "print(data[:10])\n",
    "\n",
    "\n",
    "data = SELFIES_CODER.get_encoded_SELFIES(data['SMILES'].to_list())\n",
    "\n",
    "print(data[2])\n",
    "\n",
    "\n",
    "# characters that are used in given SMILES dataset along with initial and stopping characters\n",
    "charset = set(\"\".join(list(data[0]))+\"!E\")\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "embed = data[3] + 5 #20\n",
    "print(str(charset))\n",
    "print(len(charset), embed)\n",
    "\n",
    "shap = smiles_train.shape[0]\n",
    "shap1 = smiles_test.shape[0]\n",
    "\n",
    "\n",
    "import json\n",
    "json = json.dumps(data[2])\n",
    "f = open(\"SELFIES_to_mol_seq.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json \n",
    "json = json.dumps(data[1])\n",
    "f = open(\"mol_seq_to_SELFIES.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json\n",
    "\n",
    "json = json.dumps(char_to_int)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"mol_seq_to_int.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json\n",
    "json1 = json.dumps(int_to_char)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"int_to_mol_seq.json\",\"w\")\n",
    "f.write(json1)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#vectorization of molecular sequence code\n",
    "def vectorize(smiles, shap):\n",
    "        one_hot =  np.zeros((shap, embed , len(charset)),dtype=np.int8)\n",
    "        for i,smile in enumerate(smiles):\n",
    "            #encode the startchar\n",
    "            one_hot[i,0,char_to_int[\"!\"]] = 1\n",
    "            #encode the rest of the chars\n",
    "            for j,c in enumerate(smile):\n",
    "                one_hot[i,j+1,char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[i,len(smile)+1:,char_to_int[\"E\"]] = 1\n",
    "        #Return two, one for input and the other for output\n",
    "        return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
    "smiles_train_ = data[0][:int(shap)]\n",
    "smiles_test_ = data[0][int(shap):int(shap+shap1)]\n",
    "X_train, Y_train = vectorize(smiles_train_, shap)\n",
    "X_test,Y_test = vectorize(smiles_test_, shap1)\n",
    "print(smiles_train.iloc[0])\n",
    "\n",
    "\n",
    "#going back from vectorized form to redable string\n",
    "string_test = \"\".join([int_to_char[idx] for idx in np.argmax(X_train[0,:,:], axis=1)])\n",
    "print(string_test)\n",
    "\n",
    "decoded = SELFIES_CODER.convert_back_to_SEFLIES(string_test, data[1])\n",
    "print(decoded)\n",
    "\n",
    "decoded_two = SELFIES_CODER.convert_back_to_SEFLIES(smiles_train_[0], data[1])\n",
    "print(decoded_two)\n",
    "\n",
    "print(\"Correct encoding-decoding: \"+str(decoded == decoded_two))\n",
    "\n",
    "smiles_train_[0]\n",
    "#Import Keras objects\n",
    "#LSTM cells special kinds of neural network units that are designed to keep an internal state for longer iterations\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras import regularizers\n",
    "input_shape = X_train.shape[1:]\n",
    "output_dim = Y_train.shape[-1]\n",
    "latent_dim = 128\n",
    "lstm_dim = 128\n",
    "\n",
    "\n",
    "\n",
    "#encoder-decoder architecture\n",
    "unroll = False\n",
    "encoder_inputs = Input(shape=input_shape)\n",
    "encoder = LSTM(lstm_dim, return_state=True,\n",
    "                unroll=unroll)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "states = Concatenate(axis=-1)([state_h, state_c])\n",
    "neck = Dense(latent_dim, activation=\"relu\")\n",
    "neck_outputs = neck(states)\n",
    "\n",
    "\n",
    "\n",
    "decode_h = Dense(lstm_dim, activation=\"relu\")\n",
    "decode_c = Dense(lstm_dim, activation=\"relu\")\n",
    "state_h_decoded =  decode_h(neck_outputs)\n",
    "state_c_decoded =  decode_c(neck_outputs)\n",
    "encoder_states = [state_h_decoded, state_c_decoded]\n",
    "decoder_inputs = Input(shape=input_shape)\n",
    "decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll\n",
    "                   )\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#Define the model, that inputs the training vector for two places, and predicts one character ahead of the input\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "h = History()\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=15, min_lr=0.000001, verbose=1, min_delta=1e-5) #patience=10? #epsilon=min_delta\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "opt=Adam(learning_rate=0.005) #Default 0.001\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "start_MODEL = time.time()\n",
    "model.fit([X_train,X_train],Y_train, epochs=200, batch_size=256, shuffle=True, callbacks=[h, rlr], validation_data=([X_test,X_test],Y_test)) #100\n",
    "end_MODEL = time.time()\n",
    "print(\"The time of execution of above program is :\", end_MODEL-start_MODEL)\n",
    "\n",
    "\n",
    "import pickle\n",
    "f = open(\"Neural_network_history.pickle\",\"wb\")\n",
    "pickle.dump(h.history, f)\n",
    "\n",
    "\n",
    "smiles_to_latent_model = Model(encoder_inputs, neck_outputs)\n",
    "\n",
    "smiles_to_latent_model.save(\"mol_seq2lat.h5\")\n",
    "\n",
    "\n",
    "latent_input = Input(shape=(latent_dim,))\n",
    "#reuse_layers\n",
    "state_h_decoded_2 =  decode_h(latent_input)\n",
    "state_c_decoded_2 =  decode_c(latent_input)\n",
    "latent_to_states_model = Model(latent_input, [state_h_decoded_2, state_c_decoded_2])\n",
    "latent_to_states_model.save(\"lat2state.h5\")\n",
    "\n",
    "\n",
    "#Last one is special, we need to change it to stateful, and change the input shape\n",
    "inf_decoder_inputs = Input(batch_shape=(1, 1, input_shape[1]))\n",
    "inf_decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll,\n",
    "                    stateful=True\n",
    "                   )\n",
    "inf_decoder_outputs = inf_decoder_lstm(inf_decoder_inputs)\n",
    "inf_decoder_dense = Dense(output_dim, activation='softmax')\n",
    "inf_decoder_outputs = inf_decoder_dense(inf_decoder_outputs)\n",
    "sample_model = Model(inf_decoder_inputs, inf_decoder_outputs)\n",
    "\n",
    "\n",
    "#Transfer Weights\n",
    "for i in range(1,3):\n",
    "    sample_model.layers[i].set_weights(model.layers[i+6].get_weights())\n",
    "sample_model.save(\"samplemodel.h5\")\n",
    "\n",
    "\n",
    "\n",
    "print(sample_model.summary())\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"The time of execution of above program is :\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3f55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #if the history file is empty\n",
    "f = open(\"Neural_network_history.pickle\",\"wb\")\n",
    "pickle.dump(h.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c115c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
