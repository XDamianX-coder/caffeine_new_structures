{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae12710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "My current directory is : C:\\Users\\aleks\\Documents\\GitHub\\caffeine_new_structures\\Neural_network\n",
      "My directory name is : Neural_network\n",
      "(295965,)\n",
      "(32886,)\n",
      "                                              SMILES\n",
      "0  C(\\C(=O)OC)(S)=C/C([N+1]([O-1])=O)=CN(CCCCCCCC...\n",
      "1  O=C1N(C(N(C=2N=C(SC3=NC4=C(C=CC=C4)N3CC=5N=NN(...\n",
      "2       O=C1N(C=2N=C(N(C=2C(N1C)=O)C)SC(=S)N3CCCC3)C\n",
      "3  C1(N(C(=O)CC1)CCCCNC=2N(C)C=3C(N(C(=O)N(C=3N=2...\n",
      "4          CN1C2=C(N=C1NCCCNC(=O)C)N(C)C(N(C)C2=O)=O\n",
      "5     C1(N(C)C(=O)N(C)C=2N(C(N(CCN(C)C)C)=NC1=2)C)=O\n",
      "6  N=1C2=C(C(=O)N(C(=O)N2C)C)N(C)C=1N(CCCN(C=C([N...\n",
      "7  C(CCN1C(CCC1=O)=O)NC=2N(C=3C(=O)N(C(=O)N(C=3N=...\n",
      "8  O=C1N(C2=C(C(N1C)=O)N(C)C(=N2)NCCNC(=O)CSC(=S)...\n",
      "9  O=C1N(C)C(=O)C=2N(C(=NC=2N1C)SC=3N(CC4=CN(CCCC...\n",
      "All works correct, encoding leads to the same data during decoding...\n",
      "{'[N+1]': '#', '[=Branch2]': '$', '[=O]': '%', '[Branch1]': '&', '[/C]': \"'\", '[#Branch2]': '(', '[#C]': ')', '[O]': '*', '[N]': '+', '[=N+1]': ',', '[N-1]': '-', '[Ring1]': '.', '[=Ring1]': '/', '[\\\\N]': '0', '[=N]': '1', '[/S]': '2', '[NH1]': '3', '[=S]': '4', '[O-1]': '5', '[=C]': '6', '[Branch2]': '7', '[\\\\S]': '8', '[=Ring2]': '9', '[C]': ':', '[/N]': ';', '[\\\\C]': '<', '[S]': '=', '[P]': '>', '[=Branch1]': '@', '[#Branch1]': 'A', '[Ring2]': 'B', '[=N-1]': 'C'}\n",
      "{'B', '+', '$', \"'\", '8', '(', 'E', '%', '#', '3', '.', '<', '*', ',', '1', '&', ';', '6', '>', ')', '5', '7', '/', '-', '2', '9', '0', 'C', ':', '!', '@', '4', '=', 'A'}\n",
      "34 88\n",
      "C1(=NC=2N(C(=O)N(C)C(C=2N1C)=O)C)SC3=NC4=C(C=CC=C4)N3CC5=CN(CCCN=[N+1]=[N-1])N=N5\n",
      "!:&A<@:%*:&:=6'&@#&:5%6+7B7::::::::+&:::1:+7.B:@:%+&+:@:%:/7+.*::::EEEEEEEEEEEEEEEEEEEE\n",
      "[C][Branch1][#Branch1][\\C][=Branch1][C][=O][O][C][Branch1][C][S][=C][/C][Branch1][=Branch1][N+1][Branch1][C][O-1][=O][=C][N][Branch2][Ring2][Branch2][C][C][C][C][C][C][C][C][N][Branch1][C][C][C][=N][C][N][Branch2][Ring1][Ring2][C][=Branch1][C][=O][N][Branch1][N][C][=Branch1][C][=O][C][=Ring1][Branch2][N][Ring1][O][C][C][C][C]\n",
      "[C][Branch1][#Branch1][\\C][=Branch1][C][=O][O][C][Branch1][C][S][=C][/C][Branch1][=Branch1][N+1][Branch1][C][O-1][=O][=C][N][Branch2][Ring2][Branch2][C][C][C][C][C][C][C][C][N][Branch1][C][C][C][=N][C][N][Branch2][Ring1][Ring2][C][=Branch1][C][=O][N][Branch1][N][C][=Branch1][C][=O][C][=Ring1][Branch2][N][Ring1][O][C][C][C][C]\n",
      "Correct encoding-decoding: True\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 87, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 83456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 87, 34)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 87, 128)      83456       input_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 87, 34)       4386        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 237,218\n",
      "Trainable params: 237,218\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "1157/1157 [==============================] - 35s 25ms/step - loss: 0.5867 - val_loss: 0.3117\n",
      "Epoch 2/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.2545 - val_loss: 0.2322\n",
      "Epoch 3/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.2168 - val_loss: 0.2074\n",
      "Epoch 4/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1995 - val_loss: 0.1945\n",
      "Epoch 5/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1895 - val_loss: 0.1900\n",
      "Epoch 6/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1841 - val_loss: 0.1817\n",
      "Epoch 7/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1783 - val_loss: 0.1804\n",
      "Epoch 8/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.1758 - val_loss: 0.1759\n",
      "Epoch 9/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1723 - val_loss: 0.1721\n",
      "Epoch 10/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.1684 - val_loss: 0.1714\n",
      "Epoch 11/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.1656 - val_loss: 0.1757\n",
      "Epoch 12/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.1626 - val_loss: 0.1662\n",
      "Epoch 13/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1592 - val_loss: 0.1587\n",
      "Epoch 14/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1551 - val_loss: 0.1553\n",
      "Epoch 15/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.1477 - val_loss: 0.1467\n",
      "Epoch 16/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1363 - val_loss: 0.1343\n",
      "Epoch 17/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.1284 - val_loss: 0.1404\n",
      "Epoch 18/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1204 - val_loss: 0.1173\n",
      "Epoch 19/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.1119 - val_loss: 0.1268\n",
      "Epoch 20/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.1065 - val_loss: 0.1129\n",
      "Epoch 21/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0999 - val_loss: 0.1007\n",
      "Epoch 22/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0944 - val_loss: 0.1000\n",
      "Epoch 23/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0919 - val_loss: 0.0924\n",
      "Epoch 24/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0855 - val_loss: 0.0868\n",
      "Epoch 25/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0878 - val_loss: 0.0856\n",
      "Epoch 26/200\n",
      "1157/1157 [==============================] - 26s 22ms/step - loss: 0.0795 - val_loss: 0.0939\n",
      "Epoch 27/200\n",
      "1157/1157 [==============================] - 26s 22ms/step - loss: 0.0785 - val_loss: 0.0833\n",
      "Epoch 28/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0763 - val_loss: 0.0769\n",
      "Epoch 29/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0746 - val_loss: 0.0827\n",
      "Epoch 30/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0728 - val_loss: 0.0740\n",
      "Epoch 31/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0698 - val_loss: 0.0771\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0708 - val_loss: 0.0689\n",
      "Epoch 33/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0664 - val_loss: 0.0687\n",
      "Epoch 34/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0647 - val_loss: 0.0651\n",
      "Epoch 35/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0635 - val_loss: 0.0752\n",
      "Epoch 36/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0618 - val_loss: 0.0628\n",
      "Epoch 37/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0604 - val_loss: 0.0600\n",
      "Epoch 38/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0591 - val_loss: 0.0614\n",
      "Epoch 39/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0564 - val_loss: 0.0597\n",
      "Epoch 40/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0608 - val_loss: 0.0582\n",
      "Epoch 41/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0526 - val_loss: 0.0559\n",
      "Epoch 42/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0535 - val_loss: 0.0568\n",
      "Epoch 43/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0522 - val_loss: 0.0530\n",
      "Epoch 44/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0510 - val_loss: 0.0638\n",
      "Epoch 45/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0511 - val_loss: 0.0533\n",
      "Epoch 46/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0489 - val_loss: 0.0557\n",
      "Epoch 47/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0515 - val_loss: 0.0523\n",
      "Epoch 48/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0472 - val_loss: 0.0491\n",
      "Epoch 49/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0463 - val_loss: 0.0486\n",
      "Epoch 50/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0445 - val_loss: 0.0524\n",
      "Epoch 51/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0429 - val_loss: 0.0406\n",
      "Epoch 52/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0399 - val_loss: 0.0510\n",
      "Epoch 53/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0381 - val_loss: 0.0466\n",
      "Epoch 54/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0382 - val_loss: 0.0488\n",
      "Epoch 55/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0376 - val_loss: 0.0450\n",
      "Epoch 56/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0369 - val_loss: 0.0538\n",
      "Epoch 57/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0364 - val_loss: 0.0358\n",
      "Epoch 58/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0351 - val_loss: 0.0384\n",
      "Epoch 59/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0360 - val_loss: 0.0462\n",
      "Epoch 60/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0386 - val_loss: 0.0376\n",
      "Epoch 61/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0337 - val_loss: 0.0376\n",
      "Epoch 62/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0348 - val_loss: 0.0341\n",
      "Epoch 63/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0334 - val_loss: 0.0356\n",
      "Epoch 64/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0332 - val_loss: 0.0337\n",
      "Epoch 65/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0351 - val_loss: 0.0384\n",
      "Epoch 66/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0315 - val_loss: 0.0405\n",
      "Epoch 67/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0343 - val_loss: 0.0467\n",
      "Epoch 68/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0320 - val_loss: 0.0347\n",
      "Epoch 69/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0343 - val_loss: 0.0378\n",
      "Epoch 70/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0313 - val_loss: 0.0325\n",
      "Epoch 71/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0308 - val_loss: 0.0416\n",
      "Epoch 72/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0311 - val_loss: 0.0537\n",
      "Epoch 73/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0296 - val_loss: 0.0345\n",
      "Epoch 74/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0346 - val_loss: 0.0293\n",
      "Epoch 75/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0271 - val_loss: 0.0387\n",
      "Epoch 76/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0320 - val_loss: 0.0296\n",
      "Epoch 77/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0280 - val_loss: 0.0274\n",
      "Epoch 78/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0282 - val_loss: 0.0318\n",
      "Epoch 79/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0288 - val_loss: 0.0321\n",
      "Epoch 80/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0283 - val_loss: 0.0285\n",
      "Epoch 81/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0279 - val_loss: 0.0323\n",
      "Epoch 82/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0300 - val_loss: 0.0294\n",
      "Epoch 83/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0286 - val_loss: 0.0259\n",
      "Epoch 84/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0289 - val_loss: 0.0277\n",
      "Epoch 85/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0332 - val_loss: 0.0270\n",
      "Epoch 86/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0249 - val_loss: 0.0288\n",
      "Epoch 87/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0264 - val_loss: 0.0347\n",
      "Epoch 88/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0272 - val_loss: 0.0411\n",
      "Epoch 89/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0299 - val_loss: 0.0260\n",
      "Epoch 90/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0251 - val_loss: 0.0326\n",
      "Epoch 91/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0262 - val_loss: 0.0286\n",
      "Epoch 92/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0308 - val_loss: 0.0230\n",
      "Epoch 93/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0264 - val_loss: 0.0252\n",
      "Epoch 94/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0261 - val_loss: 0.0250\n",
      "Epoch 95/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0251 - val_loss: 0.0276\n",
      "Epoch 96/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0256 - val_loss: 0.0291\n",
      "Epoch 97/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0250 - val_loss: 0.0354\n",
      "Epoch 98/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0281 - val_loss: 0.0401\n",
      "Epoch 99/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0254 - val_loss: 0.0292\n",
      "Epoch 100/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0252 - val_loss: 0.0243\n",
      "Epoch 101/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0290 - val_loss: 0.0226\n",
      "Epoch 102/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0230 - val_loss: 0.0267\n",
      "Epoch 103/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0280 - val_loss: 0.0263\n",
      "Epoch 104/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0221 - val_loss: 0.0293\n",
      "Epoch 105/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0237 - val_loss: 0.0281\n",
      "Epoch 106/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0306 - val_loss: 0.0228\n",
      "Epoch 107/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0227 - val_loss: 0.0300\n",
      "Epoch 108/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0235 - val_loss: 0.0268\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0229 - val_loss: 0.0314\n",
      "Epoch 110/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0249 - val_loss: 0.0409\n",
      "Epoch 111/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0227 - val_loss: 0.0327\n",
      "Epoch 112/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0238 - val_loss: 0.0298\n",
      "Epoch 113/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0279 - val_loss: 0.0290\n",
      "Epoch 114/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0285 - val_loss: 0.0248\n",
      "Epoch 115/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0231 - val_loss: 0.0239\n",
      "Epoch 116/200\n",
      "1157/1157 [==============================] - 26s 22ms/step - loss: 0.0221 - val_loss: 0.0251\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 117/200\n",
      "1157/1157 [==============================] - 27s 24ms/step - loss: 0.0135 - val_loss: 0.0149\n",
      "Epoch 118/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 119/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0142 - val_loss: 0.0153\n",
      "Epoch 120/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 121/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0141 - val_loss: 0.0146\n",
      "Epoch 122/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 123/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0135 - val_loss: 0.0147\n",
      "Epoch 124/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0131 - val_loss: 0.0151\n",
      "Epoch 125/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0130 - val_loss: 0.0188\n",
      "Epoch 126/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0133 - val_loss: 0.0146\n",
      "Epoch 127/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 128/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 129/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0119 - val_loss: 0.0147\n",
      "Epoch 130/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 131/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0127 - val_loss: 0.0145\n",
      "Epoch 132/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0122 - val_loss: 0.0144\n",
      "Epoch 133/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 134/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 135/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0124 - val_loss: 0.0155\n",
      "Epoch 136/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0124 - val_loss: 0.0146\n",
      "Epoch 137/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 138/200\n",
      "1157/1157 [==============================] - 26s 23ms/step - loss: 0.0116 - val_loss: 0.0146\n",
      "Epoch 139/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 140/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0125 - val_loss: 0.0215\n",
      "Epoch 141/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0122 - val_loss: 0.0142\n",
      "Epoch 142/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0117 - val_loss: 0.0138\n",
      "Epoch 143/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 144/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0115 - val_loss: 0.0179\n",
      "Epoch 145/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0148 - val_loss: 0.0121\n",
      "Epoch 146/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0113 - val_loss: 0.0135\n",
      "Epoch 147/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0116 - val_loss: 0.0192\n",
      "Epoch 148/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0150 - val_loss: 0.0117\n",
      "Epoch 149/200\n",
      "1157/1157 [==============================] - 28s 25ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 150/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0121 - val_loss: 0.0122\n",
      "Epoch 151/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0162 - val_loss: 0.0120\n",
      "Epoch 152/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 153/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 154/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 155/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0128 - val_loss: 0.0113\n",
      "Epoch 156/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0115 - val_loss: 0.0157\n",
      "Epoch 157/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0121 - val_loss: 0.0134\n",
      "Epoch 158/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 159/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 160/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0114 - val_loss: 0.0344\n",
      "Epoch 161/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 162/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 163/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0118 - val_loss: 0.0248\n",
      "Epoch 164/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 165/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0114 - val_loss: 0.0143\n",
      "Epoch 166/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 167/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0114 - val_loss: 0.0145\n",
      "Epoch 168/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0112 - val_loss: 0.0126\n",
      "Epoch 169/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 170/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0112 - val_loss: 0.0136\n",
      "Epoch 171/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0106 - val_loss: 0.0148\n",
      "Epoch 172/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 173/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 174/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0084 - val_loss: 0.0248\n",
      "Epoch 175/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0096 - val_loss: 0.0081\n",
      "Epoch 176/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0096 - val_loss: 0.0099\n",
      "Epoch 177/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0122 - val_loss: 0.0081\n",
      "Epoch 178/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0083 - val_loss: 0.0105\n",
      "Epoch 179/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0098 - val_loss: 0.0104\n",
      "Epoch 180/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0098 - val_loss: 0.0177\n",
      "Epoch 181/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0079 - val_loss: 0.0102\n",
      "Epoch 182/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0088 - val_loss: 0.0115\n",
      "Epoch 183/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 184/200\n",
      "1157/1157 [==============================] - 27s 23ms/step - loss: 0.0073 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 186/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0081 - val_loss: 0.0096\n",
      "Epoch 187/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0091 - val_loss: 0.0117\n",
      "Epoch 188/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0089 - val_loss: 0.0384\n",
      "Epoch 189/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0087 - val_loss: 0.0136\n",
      "Epoch 190/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0126 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00190: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 191/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 192/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0046 - val_loss: 0.0062\n",
      "Epoch 193/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0050 - val_loss: 0.0068\n",
      "Epoch 194/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 195/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0048 - val_loss: 0.0062\n",
      "Epoch 196/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0048 - val_loss: 0.0098\n",
      "Epoch 197/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0049 - val_loss: 0.0095\n",
      "Epoch 198/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 199/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0044 - val_loss: 0.0064\n",
      "Epoch 200/200\n",
      "1157/1157 [==============================] - 28s 24ms/step - loss: 0.0047 - val_loss: 0.0062\n",
      "The time of execution of above program is : 5482.803331851959\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(1, 1, 34)]              0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, 1, 128)               83456     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 1, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 87,842\n",
      "Trainable params: 87,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "The time of execution of above program is : 5681.301037788391\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "SELFIES_coder_path = Path(\"../SELFIES_coder\")\n",
    "sys.path.append(SELFIES_coder_path.as_posix())\n",
    "import SELFIES_coder as SELFIES_CODER\n",
    "import selfies as sf\n",
    "\n",
    "SELFIES_CODER.TEST()\n",
    "\n",
    "# Inital time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Know where user actually is\n",
    "directory_path = os.getcwd()\n",
    "print(\"My current directory is : \" + directory_path)\n",
    "folder_name = os.path.basename(directory_path)\n",
    "print(\"My directory name is : \" + folder_name)\n",
    "\n",
    "#load file with smiles codes to train and test (in this case one column is enough)\n",
    "smifile = ('../Data/training_data_smiles_caffeine.parquet')\n",
    "\n",
    "data = pd.read_parquet(smifile)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "smiles_train, smiles_test = train_test_split(data[\"SMILES\"], test_size=0.1, train_size=0.9, random_state=42)\n",
    "print(smiles_train.shape)\n",
    "print(smiles_test.shape)\n",
    "\n",
    "\n",
    "print(data[:10])\n",
    "\n",
    "\n",
    "data = SELFIES_CODER.get_encoded_SELFIES(data['SMILES'].to_list())\n",
    "\n",
    "print(data[2])\n",
    "\n",
    "\n",
    "# characters that are used in given SMILES dataset along with initial and stopping characters\n",
    "charset = set(\"\".join(list(data[0]))+\"!E\")\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "embed = data[3] + 5 #20\n",
    "print(str(charset))\n",
    "print(len(charset), embed)\n",
    "\n",
    "shap = smiles_train.shape[0]\n",
    "shap1 = smiles_test.shape[0]\n",
    "\n",
    "\n",
    "import json\n",
    "json = json.dumps(data[2])\n",
    "f = open(\"SELFIES_to_mol_seq.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json \n",
    "json = json.dumps(data[1])\n",
    "f = open(\"mol_seq_to_SELFIES.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json\n",
    "\n",
    "json = json.dumps(char_to_int)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"mol_seq_to_int.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "import json\n",
    "json1 = json.dumps(int_to_char)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"int_to_mol_seq.json\",\"w\")\n",
    "f.write(json1)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#vectorization of molecular sequence code\n",
    "def vectorize(smiles, shap):\n",
    "        one_hot =  np.zeros((shap, embed , len(charset)),dtype=np.int8)\n",
    "        for i,smile in enumerate(smiles):\n",
    "            #encode the startchar\n",
    "            one_hot[i,0,char_to_int[\"!\"]] = 1\n",
    "            #encode the rest of the chars\n",
    "            for j,c in enumerate(smile):\n",
    "                one_hot[i,j+1,char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[i,len(smile)+1:,char_to_int[\"E\"]] = 1\n",
    "        #Return two, one for input and the other for output\n",
    "        return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
    "smiles_train_ = data[0][:int(shap)]\n",
    "smiles_test_ = data[0][int(shap):int(shap+shap1)]\n",
    "X_train, Y_train = vectorize(smiles_train_, shap)\n",
    "X_test,Y_test = vectorize(smiles_test_, shap1)\n",
    "print(smiles_train.iloc[0])\n",
    "\n",
    "\n",
    "#going back from vectorized form to redable string\n",
    "string_test = \"\".join([int_to_char[idx] for idx in np.argmax(X_train[0,:,:], axis=1)])\n",
    "print(string_test)\n",
    "\n",
    "decoded = SELFIES_CODER.convert_back_to_SEFLIES(string_test, data[1])\n",
    "print(decoded)\n",
    "\n",
    "decoded_two = SELFIES_CODER.convert_back_to_SEFLIES(smiles_train_[0], data[1])\n",
    "print(decoded_two)\n",
    "\n",
    "print(\"Correct encoding-decoding: \"+str(decoded == decoded_two))\n",
    "\n",
    "smiles_train_[0]\n",
    "#Import Keras objects\n",
    "#LSTM cells special kinds of neural network units that are designed to keep an internal state for longer iterations\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras import regularizers\n",
    "input_shape = X_train.shape[1:]\n",
    "output_dim = Y_train.shape[-1]\n",
    "latent_dim = 128\n",
    "lstm_dim = 128\n",
    "\n",
    "\n",
    "\n",
    "#encoder-decoder architecture\n",
    "unroll = False\n",
    "encoder_inputs = Input(shape=input_shape)\n",
    "encoder = LSTM(lstm_dim, return_state=True,\n",
    "                unroll=unroll)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "states = Concatenate(axis=-1)([state_h, state_c])\n",
    "neck = Dense(latent_dim, activation=\"relu\")\n",
    "neck_outputs = neck(states)\n",
    "\n",
    "\n",
    "\n",
    "decode_h = Dense(lstm_dim, activation=\"relu\")\n",
    "decode_c = Dense(lstm_dim, activation=\"relu\")\n",
    "state_h_decoded =  decode_h(neck_outputs)\n",
    "state_c_decoded =  decode_c(neck_outputs)\n",
    "encoder_states = [state_h_decoded, state_c_decoded]\n",
    "decoder_inputs = Input(shape=input_shape)\n",
    "decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll\n",
    "                   )\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#Define the model, that inputs the training vector for two places, and predicts one character ahead of the input\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "h = History()\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=15, min_lr=0.000001, verbose=1, min_delta=1e-5) #epsilon=min_delta\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "opt=Adam(learning_rate=0.005) #Default 0.001\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "start_MODEL = time.time()\n",
    "model.fit([X_train,X_train],Y_train, epochs=200, batch_size=256, shuffle=True, callbacks=[h, rlr], validation_data=([X_test,X_test],Y_test)) #100\n",
    "end_MODEL = time.time()\n",
    "print(\"The time of execution of above program is :\", end_MODEL-start_MODEL)\n",
    "\n",
    "\n",
    "import pickle\n",
    "f = open(\"Neural_network_history.pickle\",\"wb\")\n",
    "pickle.dump(h.history, f)\n",
    "\n",
    "\n",
    "smiles_to_latent_model = Model(encoder_inputs, neck_outputs)\n",
    "\n",
    "smiles_to_latent_model.save(\"mol_seq2lat.h5\")\n",
    "\n",
    "\n",
    "latent_input = Input(shape=(latent_dim,))\n",
    "#reuse_layers\n",
    "state_h_decoded_2 =  decode_h(latent_input)\n",
    "state_c_decoded_2 =  decode_c(latent_input)\n",
    "latent_to_states_model = Model(latent_input, [state_h_decoded_2, state_c_decoded_2])\n",
    "latent_to_states_model.save(\"lat2state.h5\")\n",
    "\n",
    "\n",
    "#Last one is special, we need to change it to stateful, and change the input shape\n",
    "inf_decoder_inputs = Input(batch_shape=(1, 1, input_shape[1]))\n",
    "inf_decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll,\n",
    "                    stateful=True\n",
    "                   )\n",
    "inf_decoder_outputs = inf_decoder_lstm(inf_decoder_inputs)\n",
    "inf_decoder_dense = Dense(output_dim, activation='softmax')\n",
    "inf_decoder_outputs = inf_decoder_dense(inf_decoder_outputs)\n",
    "sample_model = Model(inf_decoder_inputs, inf_decoder_outputs)\n",
    "\n",
    "\n",
    "#Transfer Weights\n",
    "for i in range(1,3):\n",
    "    sample_model.layers[i].set_weights(model.layers[i+6].get_weights())\n",
    "sample_model.save(\"samplemodel.h5\")\n",
    "\n",
    "\n",
    "\n",
    "print(sample_model.summary())\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"The time of execution of above program is :\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3f55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #if the history file is empty\n",
    "f = open(\"Neural_network_history.pickle\",\"wb\")\n",
    "pickle.dump(h.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c115c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
